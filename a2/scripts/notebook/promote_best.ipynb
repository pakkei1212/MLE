{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb152799-3a22-4e94-ad32-1f27ab0133a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripts/promote_best.py\n",
    "import argparse, os, json\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import pandas as pd\n",
    "\n",
    "def main(args):\n",
    "    # Resolve tracking URI: arg > env > default\n",
    "    tracking_uri = args.tracking_uri or os.getenv(\"MLFLOW_TRACKING_URI\", \"http://mlflow:5000\")\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    client = MlflowClient()\n",
    "\n",
    "    model_name  = args.model_name\n",
    "    train_date  = args.train_date.strip()           # \"YYYY-MM-DD\"\n",
    "    metric_key  = args.metric\n",
    "    stage       = args.stage\n",
    "    archive_old = bool(args.archive_existing)\n",
    "    dry_run     = bool(args.dry_run)\n",
    "\n",
    "    # 1) Find experiment\n",
    "    exp = mlflow.get_experiment_by_name(args.experiment)\n",
    "    if exp is None:\n",
    "        print(f\"[PROMOTE] Experiment '{args.experiment}' not found at {tracking_uri}.\")\n",
    "        return 1\n",
    "\n",
    "    # 2) Search runs for this train_date, order by the chosen metric desc\n",
    "    flt = (\n",
    "        f\"tags.train_date = '{train_date}' \"\n",
    "        \"and attributes.status = 'FINISHED'\"\n",
    "    )\n",
    "\n",
    "    runs = mlflow.search_runs(\n",
    "        experiment_ids=[exp.experiment_id],\n",
    "        filter_string=flt,\n",
    "        order_by=[f\"metrics.{metric_key} DESC\"],  # ordering is fine even if some are NaN\n",
    "        max_results=200,\n",
    "    )\n",
    "\n",
    "    metric_series = pd.to_numeric(runs.get(f\"metrics.{metric_key}\"), errors=\"coerce\")\n",
    "    runs = runs[metric_series.notna()]\n",
    "\n",
    "    if runs.empty:\n",
    "        print(f\"[PROMOTE] No runs found for train_date={train_date} with metric '{metric_key}'.\")\n",
    "        return 2\n",
    "\n",
    "    best = runs.iloc[0]\n",
    "    run_id = best.run_id\n",
    "    best_metric = float(best[f\"metrics.{metric_key}\"])\n",
    "    print(f\"[PROMOTE] Best run for {train_date}: run_id={run_id} | {metric_key}={best_metric:.6f}\")\n",
    "\n",
    "    # Optional guardrails (e.g., don't promote if test AUC is too low)\n",
    "    if args.min_test_auc is not None:\n",
    "        mt = best.get(\"metrics.auc_test\")\n",
    "        if mt is None or float(mt) < args.min_test_auc:\n",
    "            print(f\"[PROMOTE] Guard failed: auc_test={mt} < min_test_auc={args.min_test_auc}. Aborting.\")\n",
    "            return 3\n",
    "\n",
    "    # Collect a compact metrics/params snapshot for description\n",
    "    keys_want = [\n",
    "        \"auc_train\",\"auc_test\",\"auc_oot\",\n",
    "        \"gini_train\",\"gini_test\",\"gini_oot\",\n",
    "        \"accuracy_test\",\"precision_weighted_test\",\"recall_weighted_test\",\"f1_weighted_test\",\n",
    "    ]\n",
    "    metrics_snapshot = {}\n",
    "    for k in keys_want:\n",
    "        v = best.get(f\"metrics.{k}\")\n",
    "        if v is not None:\n",
    "            metrics_snapshot[k] = float(v)\n",
    "\n",
    "    params_snapshot = {}\n",
    "    for col in best.index:\n",
    "        if col.startswith(\"params.\"):\n",
    "            params_snapshot[col.split(\"params.\", 1)[1]] = best[col]\n",
    "\n",
    "    # 3) Ensure there is a Model Version for this run\n",
    "    # Discover the logged artifact path from the run's tags (robust to any name)\n",
    "    run = client.get_run(run_id)\n",
    "    hist_tag = run.data.tags.get(\"mlflow.log-model.history\")\n",
    "\n",
    "    artifact_path = None\n",
    "    if hist_tag:\n",
    "        try:\n",
    "            hist = json.loads(hist_tag)\n",
    "            # Prefer the most recent logged model that has actual artifacts\n",
    "            for entry in reversed(hist):\n",
    "                cand = entry.get(\"artifact_path\")\n",
    "                if not cand:\n",
    "                    continue\n",
    "                # verify this artifact path exists for this run\n",
    "                try:\n",
    "                    client.list_artifacts(run_id, cand)\n",
    "                    artifact_path = cand\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"[PROMOTE] Could not parse mlflow.log-model.history: {e}\")\n",
    "\n",
    "    # Fallback to the conventional 'model' path if nothing found\n",
    "    if not artifact_path:\n",
    "        artifact_path = \"model\"\n",
    "\n",
    "    src_uri = f\"runs:/{run_id}/{artifact_path}\"\n",
    "    print(f\"[PROMOTE] Using artifact_path='{artifact_path}' -> {src_uri}\")\n",
    "\n",
    "    # If already registered for this run, reuse it (idempotent)\n",
    "    existing = [\n",
    "        mv for mv in client.search_model_versions(f\"name='{model_name}'\")\n",
    "        if mv.run_id == run_id and mv.source == src_uri\n",
    "    ]\n",
    "    if existing:\n",
    "        mv = sorted(existing, key=lambda m: int(m.version))[-1]\n",
    "        print(f\"[PROMOTE] Reusing existing model version: {model_name} v{mv.version}\")\n",
    "    else:\n",
    "        if dry_run:\n",
    "            print(f\"[PROMOTE][DRY] Would register model from source: {src_uri}\")\n",
    "            return 0\n",
    "        mv = mlflow.register_model(src_uri, model_name)\n",
    "        print(f\"[PROMOTE] Registered: {model_name} v{mv.version}\")\n",
    "\n",
    "    # 4) Update model version description with metrics/params\n",
    "    desc = {\n",
    "        \"train_date\": train_date,\n",
    "        \"opt_metric\": metric_key,\n",
    "        \"opt_metric_value\": best_metric,\n",
    "        \"run_id\": run_id,\n",
    "        \"metrics\": metrics_snapshot,\n",
    "        \"params\": params_snapshot,\n",
    "    }\n",
    "    client.update_model_version(\n",
    "        name=model_name,\n",
    "        version=mv.version,\n",
    "        description=json.dumps(desc, indent=2, sort_keys=True),\n",
    "    )\n",
    "\n",
    "    # 5) Transition stage\n",
    "    if dry_run:\n",
    "        print(f\"[PROMOTE][DRY] Would transition {model_name} v{mv.version} -> {stage} (archive_existing={archive_old})\")\n",
    "        return 0\n",
    "\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=mv.version,\n",
    "        stage=stage,\n",
    "        archive_existing_versions=archive_old,\n",
    "    )\n",
    "    print(f\"[PROMOTE] {model_name} v{mv.version} -> {stage} (archive_existing={archive_old})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
